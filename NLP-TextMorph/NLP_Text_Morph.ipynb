{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOw5FNJuEQ8pn+6f8zCWR2F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadreza-mohammadi94/NLP-Projects/blob/main/NLP-TextMorph/NLP_Text_Morph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries & Project Setup"
      ],
      "metadata": {
        "id": "xxPYyFczPDYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from typing import List, Tuple\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)\",\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "YmZzGp7dPDI1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK0gYbA_O6iJ",
        "outputId": "053e2182-7b8f-4d53-d44c-cde8530fb699"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Load NLTK Packages\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Processor"
      ],
      "metadata": {
        "id": "MOqODgl5QWuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextProcessor:\n",
        "    \"\"\" Used to process texts using NLTK \"\"\"\n",
        "    def __init__(self):\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "    def  tokenize_text(self, text: str):\n",
        "        \"\"\" For tokenization sentences \"\"\"\n",
        "        try:\n",
        "            tokens = word_tokenize(text.lower())\n",
        "            logger.info(\"Tokenization Succeed..\")\n",
        "            return tokens\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Tokenization Failed: {e}\")\n",
        "\n",
        "    def pos_tag_tokens(self, tokens: List[str]):\n",
        "        \"\"\"Tags the tokens\"\"\"\n",
        "        try:\n",
        "            tagged = pos_tag(tokens)\n",
        "            logger.info(\"Tagging Succeed...\")\n",
        "            return tagged\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Tagging Failed: {e}\")\n",
        "            return []\n",
        "\n",
        "    @staticmethod\n",
        "    def get_wordnet_pos(treebank_tag: str):\n",
        "        \"\"\" Converts Treebank tags to Wordnet \"\"\"\n",
        "        pos_mapping = {\n",
        "            'V': 'v',  # فعل\n",
        "            'N': 'n',  # اسم\n",
        "            'J': 'a',  # صفت\n",
        "            'R': 'r'   # قید\n",
        "        }\n",
        "        return pos_mapping.get(treebank_tag[0], 'n')\n",
        "\n",
        "    def lemmatize_with_pos(self, tagged_tokens: List[Tuple[str, str]]) -> List[str]:\n",
        "        \"\"\"Lemmatization Using POS\"\"\"\n",
        "        try:\n",
        "            lemmatized = [self.lemmatizer.lemmatize(word, pos=self.get_wordnet_pos(tag))\n",
        "                         for word, tag in tagged_tokens]\n",
        "            logger.info(\"Lemmatized Using POS Succeed....\")\n",
        "            return lemmatized\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Lemmatization With POS Failed: {e}\")\n",
        "            return []\n",
        "\n",
        "    def lemmatize_without_pos(self, tokens) -> List[str]:\n",
        "        \"\"\"Lemmatization without POS (Default value 'n' for pos)\"\"\"\n",
        "        try:\n",
        "            lemmatized = [self.lemmatizer.lemmatize(word) for word in tokens]\n",
        "            logger.info(\"Lemmatization Without POS Succeed...\")\n",
        "            return lemmatized\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Lemmatization Without POS Failed: {e}\")\n",
        "            return []\n",
        "\n",
        "    def remove_stopwords(self, tokens):\n",
        "        return [token for token in tokens if token not in self.stop_words and token.isalpha()]\n",
        "\n",
        "    def process_text(self, text: str):\n",
        "        \"\"\"Processing text and shows results\"\"\"\n",
        "        # Tokenization\n",
        "        tokens = self.tokenize_text(text)\n",
        "        if not tokens:\n",
        "            return {}\n",
        "\n",
        "        # Remove Stopwords\n",
        "        filtered_tokens = self.remove_stopwords(tokens)\n",
        "\n",
        "        # POS Tagging\n",
        "        tagged_tokens = self.pos_tag_tokens(filtered_tokens)\n",
        "\n",
        "        # Lemmatization\n",
        "        lemmatized_with_pos = self.lemmatize_with_pos(tagged_tokens)\n",
        "        lemmatized_without_pos = self.lemmatize_without_pos(filtered_tokens)\n",
        "\n",
        "        # Results\n",
        "        results = {\n",
        "            'Original Tokens': filtered_tokens,\n",
        "            'POS Tags': [tag for _, tag in tagged_tokens],\n",
        "            'Lemmatized with POS': lemmatized_with_pos,\n",
        "            'Lemmatized without POS': lemmatized_without_pos\n",
        "        }\n",
        "        return results"
      ],
      "metadata": {
        "id": "CBFf17cvQJUF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use TextProcessor"
      ],
      "metadata": {
        "id": "OczJT-55Z3W2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(results: dict, filepath: str = \"results.csv\", overwrite: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Displays results in a DataFrame and saves it to a file.\n",
        "\n",
        "    Args:\n",
        "        results (dict): Dictionary containing text processing results.\n",
        "        filepath (str): Path to save the DataFrame (default: 'results.csv').\n",
        "        overwrite (bool): Whether to overwrite the file if it exists (default: True).\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted string representation of the DataFrame.\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "        logger.warning(\"Nothing to show or save...\")\n",
        "        return \"\"\n",
        "\n",
        "    try:\n",
        "        # Creating DataFrame from results\n",
        "        df = pd.DataFrame(results)\n",
        "        logger.info(\"DataFrame created successfully\")\n",
        "\n",
        "        # Save Df\n",
        "        if os.path.exists(filepath) and not overwrite:\n",
        "            logger.warning(f\"File '{filepath}' already exists and overwrite is False. Skipping save.\")\n",
        "        else:\n",
        "            df.to_csv(filepath, index=False, encoding='utf-8')\n",
        "            logger.info(f\"Results saved to '{filepath}' successfully\")\n",
        "\n",
        "        # Show df\n",
        "        result_string = df.to_string(index=False)\n",
        "        print(result_string)\n",
        "\n",
        "        return result_string\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to process or save results: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "w5syL52uZyj9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    sample_text = \"\"\"\n",
        "            Reasoning models like OpenAI’s o1 have shown great promise in domains like math and physics,\n",
        "            thanks to their ability to effectively fact-check themselves by working through complex problems step by step.\n",
        "            This reasoning comes at a cost, however: higher computing and latency.\n",
        "            That’s why labs like Anthropic are pursuing “hybrid” model architectures that combine reasoning components with standard, non-reasoning elements.\n",
        "            Hybrid models can quickly answer simple questions while spending additional time considering more challenging queries.\n",
        "                \"\"\"\n",
        "    logger.info(\"Text Processing Started...\")\n",
        "\n",
        "    # Create instance of TextProcessor\n",
        "    processor = TextProcessor()\n",
        "\n",
        "    # Processing\n",
        "    results = processor.process_text(sample_text)\n",
        "\n",
        "    # Show results\n",
        "    print(display_results(results))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wBl8C_maSm5",
        "outputId": "bc892803-6258-4a16-815e-4ebd10fe4f0b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens POS Tags Lemmatized with POS Lemmatized without POS\n",
            "      reasoning      VBG              reason              reasoning\n",
            "         models      NNS               model                  model\n",
            "           like       IN                like                   like\n",
            "         openai       JJ              openai                 openai\n",
            "          shown      VBN                show                  shown\n",
            "          great       JJ               great                  great\n",
            "        promise       NN             promise                promise\n",
            "        domains      NNS              domain                 domain\n",
            "           like       IN                like                   like\n",
            "           math       NN                math                   math\n",
            "        physics      NNS              physic                 physic\n",
            "         thanks      NNS              thanks                 thanks\n",
            "        ability       NN             ability                ability\n",
            "    effectively       RB         effectively            effectively\n",
            "        working      VBG                work                working\n",
            "        complex       JJ             complex                complex\n",
            "       problems      NNS             problem                problem\n",
            "           step      VBP                step                   step\n",
            "           step       VB                step                   step\n",
            "      reasoning      VBG              reason              reasoning\n",
            "          comes      VBZ                come                   come\n",
            "           cost       NN                cost                   cost\n",
            "        however       RB             however                however\n",
            "         higher      JJR                high                 higher\n",
            "      computing      VBG             compute              computing\n",
            "        latency      RBR             latency                latency\n",
            "           labs      NNS                 lab                    lab\n",
            "           like       IN                like                   like\n",
            "      anthropic       NN           anthropic              anthropic\n",
            "       pursuing      VBG              pursue               pursuing\n",
            "         hybrid       JJ              hybrid                 hybrid\n",
            "          model       NN               model                  model\n",
            "  architectures      NNS        architecture           architecture\n",
            "        combine      VBP             combine                combine\n",
            "      reasoning      VBG              reason              reasoning\n",
            "     components      NNS           component              component\n",
            "       standard       JJ            standard               standard\n",
            "       elements      NNS             element                element\n",
            "         hybrid       JJ              hybrid                 hybrid\n",
            "         models      NNS               model                  model\n",
            "        quickly       RB             quickly                quickly\n",
            "         answer      VBP              answer                 answer\n",
            "         simple       JJ              simple                 simple\n",
            "      questions      NNS            question               question\n",
            "       spending      VBG               spend               spending\n",
            "     additional       JJ          additional             additional\n",
            "           time       NN                time                   time\n",
            "    considering      VBG            consider            considering\n",
            "    challenging      VBG           challenge            challenging\n",
            "        queries      NNS               query                  query\n",
            "Original Tokens POS Tags Lemmatized with POS Lemmatized without POS\n",
            "      reasoning      VBG              reason              reasoning\n",
            "         models      NNS               model                  model\n",
            "           like       IN                like                   like\n",
            "         openai       JJ              openai                 openai\n",
            "          shown      VBN                show                  shown\n",
            "          great       JJ               great                  great\n",
            "        promise       NN             promise                promise\n",
            "        domains      NNS              domain                 domain\n",
            "           like       IN                like                   like\n",
            "           math       NN                math                   math\n",
            "        physics      NNS              physic                 physic\n",
            "         thanks      NNS              thanks                 thanks\n",
            "        ability       NN             ability                ability\n",
            "    effectively       RB         effectively            effectively\n",
            "        working      VBG                work                working\n",
            "        complex       JJ             complex                complex\n",
            "       problems      NNS             problem                problem\n",
            "           step      VBP                step                   step\n",
            "           step       VB                step                   step\n",
            "      reasoning      VBG              reason              reasoning\n",
            "          comes      VBZ                come                   come\n",
            "           cost       NN                cost                   cost\n",
            "        however       RB             however                however\n",
            "         higher      JJR                high                 higher\n",
            "      computing      VBG             compute              computing\n",
            "        latency      RBR             latency                latency\n",
            "           labs      NNS                 lab                    lab\n",
            "           like       IN                like                   like\n",
            "      anthropic       NN           anthropic              anthropic\n",
            "       pursuing      VBG              pursue               pursuing\n",
            "         hybrid       JJ              hybrid                 hybrid\n",
            "          model       NN               model                  model\n",
            "  architectures      NNS        architecture           architecture\n",
            "        combine      VBP             combine                combine\n",
            "      reasoning      VBG              reason              reasoning\n",
            "     components      NNS           component              component\n",
            "       standard       JJ            standard               standard\n",
            "       elements      NNS             element                element\n",
            "         hybrid       JJ              hybrid                 hybrid\n",
            "         models      NNS               model                  model\n",
            "        quickly       RB             quickly                quickly\n",
            "         answer      VBP              answer                 answer\n",
            "         simple       JJ              simple                 simple\n",
            "      questions      NNS            question               question\n",
            "       spending      VBG               spend               spending\n",
            "     additional       JJ          additional             additional\n",
            "           time       NN                time                   time\n",
            "    considering      VBG            consider            considering\n",
            "    challenging      VBG           challenge            challenging\n",
            "        queries      NNS               query                  query\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uReJhJVYarDL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ih-MnMUsasat"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}