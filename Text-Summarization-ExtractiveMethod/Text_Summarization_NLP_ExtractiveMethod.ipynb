{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtA/Bp8KmOIXr5f3D0+feF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadreza-mohammadi94/NLP-Projects/blob/main/Text-Summarization-ExtractiveMethod/Text_Summarization_NLP_ExtractiveMethod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Frequency Based"
      ],
      "metadata": {
        "id": "0USLqAo3tVTY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9UHoXU5Ui6Ko",
        "outputId": "6fc90752-ee99-430c-ef09-7aaab77aadf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "import re\n",
        "import heapq\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "\n",
        "# NLTK\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download(\"wordnet\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "A litany of text summarization methods have been developed over the last several decades, so answering how text summarization works doesn’t have a single answer.\n",
        "This having been said, these methods can be classified according to their general approaches in addressing the challenge of text summarization.\n",
        "Perhaps the most clear-cut and helpful distinction is that between Extractive and Abstractive text summarization methods. Extractive methods seek to extract the most pertinent information from a text.\n",
        "Extractive text summarization is the more traditional of the two methods, in part because of their relative simplicity compared to abstractive methods.\n",
        "Abstractive methods instead seek to generate a novel body of text that accurately summarizes the original text. Already we can see how this is a more difficult problem - there is a significant degree of freedom in not being limited to simply returning a subset of the original text. This difficulty comes with an upside, though.\n",
        "Despite their relative complexity, Abstractive methods produce much more flexible and arguably faithful summaries, especially in the age of Large Language Models.\n",
        "As mentioned above, Extractive Text Summarization methods work by identifying and extracting the salient information in a text.\n",
        "The variety of Extractive methods therefore constitutes different ways of determining what information is important (and therefore should be extracted).\n",
        "For example frequency-based methods will tend to rank the sentences in a text in order of importance by how frequently different words are used.\n",
        "For each sentence, there exists a weighting term for each word in the vocabulary, where the weight is usually a function of the importance of the word itself and the frequency with which the word appears throughout the document as a whole.\n",
        "Using these weights, the importance of each sentence can then be determined and returned.\n",
        "Graph-based methods cast textual documents in the language of mathematical graphs.\n",
        "In this schema, each sentence is represented as a node, where nodes are connected if the sentences are deemed to be similar.\n",
        "What constitutes “similar” is, again, a choice of different specific algorithms and approaches.\n",
        "For example, one implementation might use a threshold on the cosine similarity between TF-IDF vectors. In general, the sentences that are globally the “most similar” to all other sentences (i.e. those with the highest centrality) in the document are considered to have the most summarizing information,\n",
        "and are therefore extracted and put into the summary.\n",
        "A notable example of a graph-based method is TextRank, a version of Google’s pagerank algorithm (which determines what results to display in Google Search) that has been adapted for summarization (instead ranking the most important sentences).\n",
        "Graph-based methods may benefit in the future from advances in Graph Neural Networks.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "aAM6J0Jgtb6U"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\s+\", \" \", text) # Removes whitespaces\n",
        "    text = re.sub(r\"\\W\", \" \", text) # Removes non alphabetic char\n",
        "    text = re.sub(r\"\\d+\", \"\", text) # Removes digits\n",
        "    words = nltk.word_tokenize(text) # Tokenization\n",
        "    words = [word for word in words if word not in stopwords.words(\"english\")] # Remove stopwords\n",
        "    words = [nltk.WordNetLemmatizer().lemmatize(word) for word in words] # Lemmatization\n",
        "    return words"
      ],
      "metadata": {
        "id": "R5ikuWsTk9BZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text = preprocess_text(text)"
      ],
      "metadata": {
        "id": "bQWXkSgflsDt"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Words frequency analysis\n",
        "word_freq = defaultdict(int)\n",
        "for word in clean_text:\n",
        "    word_freq[word] += 1\n",
        "\n",
        "# Frequency normalization\n",
        "max_freq = max(word_freq.values())\n",
        "for word in word_freq.keys():\n",
        "    word_freq[word] /= max_freq"
      ],
      "metadata": {
        "id": "uxuAkow2m7KC"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scoring Sentences\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "sentences_score = {}\n",
        "\n",
        "for sent in sentences:\n",
        "    sent_words = preprocess_text(sent)\n",
        "    for word in sent_words:\n",
        "        if word in word_freq:\n",
        "            if sent not in sentences_score:\n",
        "                sentences_score[sent]  = word_freq[word]\n",
        "            else:\n",
        "                sentences_score[sent] += word_freq[word]"
      ],
      "metadata": {
        "id": "QBvWFQkUoQg6"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_sentences = heapq.nlargest(3,\n",
        "                               sentences_score,\n",
        "                               key=sentences_score.get)"
      ],
      "metadata": {
        "id": "Vdoi37kKo3Ro"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = \" \".join(top_sentences)\n",
        "print(\"Summarization:\\n\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "e-GVUf_5pU8Y",
        "outputId": "5cd1547b-847b-41e4-dd15-ddad0a85e687"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarization:\n",
            "\n",
            "Extractive text summarization is the more traditional of the two methods, in part because of their relative simplicity compared to abstractive methods. \n",
            "A litany of text summarization methods have been developed over the last several decades, so answering how text summarization works doesn’t have a single answer. A notable example of a graph-based method is TextRank, a version of Google’s pagerank algorithm (which determines what results to display in Google Search) that has been adapted for summarization (instead ranking the most important sentences).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using TF-IDF"
      ],
      "metadata": {
        "id": "wtDPrlNVqpoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "HuzsFYt4py8G",
        "outputId": "75bc5535-95f6-4df6-f1c2-96ce14f7bee0"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = re.sub(r\"\\W\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "def summarize_text(text, num_sent=3):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    preprocessed_sentences = [preprocess(sent) for sent in sentences]\n",
        "\n",
        "    # Vectorization\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    sentence_vectors = vectorizer.fit_transform(preprocessed_sentences)\n",
        "\n",
        "    # sentences scoring\n",
        "    sentence_scores = {}\n",
        "    for i, sent in enumerate(preprocessed_sentences):\n",
        "        sentence_scores[sentences[i]] = np.mean(sentence_vectors[i].toarray())\n",
        "\n",
        "    # Chose top sentences\n",
        "    sorted_sentences = sorted(sentence_scores.items(),\n",
        "                              key = lambda x: x[1],\n",
        "                              reverse=True)\n",
        "    summary = \" \".join([sent[0] for sent in sorted_sentences[:num_sent]])\n",
        "    return summary"
      ],
      "metadata": {
        "id": "7AXJCcpwq3ja"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = summarize_text(text)\n",
        "print(\"Summarization:\\n\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MHNyd_HTsJZh",
        "outputId": "a91b53e7-ae9c-4038-fe3c-4497708af606"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarization:\n",
            "\n",
            "A notable example of a graph-based method is TextRank, a version of Google’s pagerank algorithm (which determines what results to display in Google Search) that has been adapted for summarization (instead ranking the most important sentences). Already we can see how this is a more difficult problem - there is a significant degree of freedom in not being limited to simply returning a subset of the original text. For each sentence, there exists a weighting term for each word in the vocabulary, where the weight is usually a function of the importance of the word itself and the frequency with which the word appears throughout the document as a whole.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hdtZyp3rsL64"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}