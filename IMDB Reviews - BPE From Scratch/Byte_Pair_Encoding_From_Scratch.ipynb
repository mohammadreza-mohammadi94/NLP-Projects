{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMLJXl8AZr+kFbV0c6BhXF/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadreza-mohammadi94/NLP-Projects/blob/main/IMDB%20Reviews%20-%20BPE%20From%20Scratch/Byte_Pair_Encoding_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "r6k5HW1e57g8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfA-gnCw52U5"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, Counter\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dataset"
      ],
      "metadata": {
        "id": "AMIxoyib7wxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "C5SwunKo7w_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "D0S5Hldw_DeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')  # مسیر فایل رو درست کن\n",
        "print(f\"Number Of Reviews: {len(df)}\")"
      ],
      "metadata": {
        "id": "xQuSv4m2_FEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "cTl9mRqN_dsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    words = text.split()\n",
        "    return words\n",
        "\n",
        "corpus = []\n",
        "for review in df['review']:\n",
        "    words = preprocess_text(review)\n",
        "    corpus.extend(words)\n",
        "\n",
        "print(f\"Number Of Words: {len(corpus)}\")\n",
        "print(f\"Few Samples: {corpus[:10]}\")"
      ],
      "metadata": {
        "id": "-KUXEFrM_j8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "T8WcetJV6Xhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(vocab):\n",
        "    \"\"\"\n",
        "    Compute frequency of adjacent symbol pairs in a given vocabulary.\n",
        "\n",
        "    Args:\n",
        "        vocab (dict): A dictionary where keys are space-separated strings (words or subwords)\n",
        "                      and values are their corresponding frequencies.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with symbol pairs as keys (tuples of two strings)\n",
        "              and their total frequency of co-occurrence as values.\n",
        "    \"\"\"\n",
        "    pairs = defaultdict(int)  # Store frequency of each adjacent symbol pair\n",
        "\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()  # Split the word into individual symbols (tokens/subwords)\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pair = (symbols[i], symbols[i + 1])  # Get current symbol pair\n",
        "            pairs[pair] += freq  # Increment frequency by word's occurrence count\n",
        "\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def merge_vocab(pair, vocab):\n",
        "    \"\"\"\n",
        "    Merge the most frequent symbol pair in the vocabulary.\n",
        "\n",
        "    Args:\n",
        "        pair (tuple): A tuple of two strings representing the symbol pair to merge.\n",
        "                      For example, ('l', 'o') to merge into 'lo'.\n",
        "        vocab (dict): A dictionary where keys are space-separated strings (words/subwords)\n",
        "                      and values are their frequencies.\n",
        "\n",
        "    Returns:\n",
        "        dict: A new vocabulary dictionary with the specified pair merged into a single symbol.\n",
        "    \"\"\"\n",
        "    new_vocab = {}\n",
        "    bigram = \" \".join(pair)       # e.g., ('l', 'o') -> 'l o'\n",
        "    replacement = \"\".join(pair)   # e.g., ('l', 'o') -> 'lo'\n",
        "\n",
        "    for word in vocab:\n",
        "        # Replace all occurrences of the bigram with the merged symbol\n",
        "        new_word = word.replace(bigram, replacement)\n",
        "        new_vocab[new_word] = vocab[word]  # Preserve the frequency\n",
        "\n",
        "    return new_vocab\n",
        "\n",
        "\n",
        "def learn_bpe(corpus, num_merges=1000):\n",
        "    \"\"\"\n",
        "    Learn Byte Pair Encoding (BPE) merges from a text corpus.\n",
        "\n",
        "    Args:\n",
        "        corpus (list of str): The input corpus where each item is a word or sentence.\n",
        "        num_merges (int): The maximum number of BPE merge operations to perform.\n",
        "\n",
        "    Returns:\n",
        "        tuple:\n",
        "            - vocab (dict): Final vocabulary after BPE merges, with words represented as space-separated subwords.\n",
        "            - merges (list of tuple): List of merged symbol pairs in the order they were applied.\n",
        "    \"\"\"\n",
        "    vocab = Counter()\n",
        "\n",
        "    # Initialize the vocabulary: each word is split into characters + </w> to mark end of word\n",
        "    for word in corpus:\n",
        "        word = \" \".join(list(word)) + \" </w>\"\n",
        "        vocab[word] += 1\n",
        "\n",
        "    merges = []\n",
        "\n",
        "    # Perform BPE merges up to num_merges times\n",
        "    for i in range(num_merges):\n",
        "        pairs = get_stats(vocab)  # Count frequency of symbol pairs\n",
        "        if not pairs:\n",
        "            break  # No more pairs to merge\n",
        "\n",
        "        best_pair = max(pairs, key=pairs.get)  # Most frequent pair\n",
        "        vocab = merge_vocab(best_pair, vocab)  # Merge the best pair in the vocab\n",
        "        merges.append(best_pair)\n",
        "\n",
        "        # Log progress every 100 merges\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Merge {i + 1}: {best_pair}\")\n",
        "\n",
        "    return vocab, merges\n",
        "\n",
        "\n",
        "def apply_bpe(word, merges):\n",
        "    \"\"\"\n",
        "    Applies Byte Pair Encoding (BPE) merges to a given word.\n",
        "\n",
        "    Args:\n",
        "        word (str): The input word to encode using learned BPE merges.\n",
        "        merges (list of tuple): List of symbol pairs (bigrams) in the order they were merged during BPE training.\n",
        "\n",
        "    Returns:\n",
        "        list of str: The BPE-encoded word as a list of subword tokens.\n",
        "    \"\"\"\n",
        "    # FIXED TYPO: .joint() → .join()\n",
        "    # Split word into characters and append </w> to mark end of word\n",
        "    word = \" \".join(list(word)) + \" </w>\"\n",
        "    word = word.split()\n",
        "\n",
        "    # Apply each merge operation in sequence\n",
        "    for pair in merges:\n",
        "        bigram = pair[0] + \" \" + pair[1]             # E.g., 't h'\n",
        "        replacement = pair[0] + pair[1]               # E.g., 'th'\n",
        "        word = \" \".join(word).replace(bigram, replacement).split()\n",
        "\n",
        "    return word"
      ],
      "metadata": {
        "id": "lzzyWhhe5_aM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply Functions"
      ],
      "metadata": {
        "id": "KNfGJ98RAnG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_merges = 1000\n",
        "vocab, merges = learn_bpe(corpus)\n",
        "\n",
        "print(f\"Vocab Size: {len(vocab)}\")\n",
        "print(f\"Merges: {merges[:10]}\")\n",
        "\n",
        "# Tokenization\n",
        "sample_reviews = df['review'].iloc[:3].tolist()\n",
        "tokenized_reviews = []\n",
        "\n",
        "for review in sample_reviews:\n",
        "    words = preprocess_text(review)\n",
        "    tokens = []\n",
        "    for word in words:\n",
        "        tokens.extend(apply_bpe(word, merges))\n",
        "    tokenized_reviews.append(tokens)\n",
        "\n",
        "for i, (review, tokens) in enumerate(zip(sample_reviews, tokenized_reviews)):\n",
        "    print(f\"\\Review {i+1}: {review[:100]}...\")\n",
        "    print(f\"Tokens: {tokens[:20]}...\")\n",
        "    print(f\"Number Of Tokens توکن‌ها: {len(tokens)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM0hKkW-7131",
        "outputId": "2c6956b0-98f6-4058-9064-dc6443b53fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge 100: ('c', 'om')\n",
            "Merge 200: ('stor', 'y</w>')\n",
            "Merge 300: ('s', 'pe')\n",
            "Merge 400: ('i', 've</w>')\n",
            "Merge 500: ('ma', 'y')\n",
            "Merge 600: ('z', '</w>')\n",
            "Merge 700: ('hu', 'man</w>')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis of Results\n",
        "unique_tokens = set()\n",
        "for word in vocab:\n",
        "    unique_tokens.update(word.split())\n",
        "print(f\"Unique Tokens: {len(unique_tokens)}\")\n",
        "\n",
        "avg_tokens = sum(len(tokens) for tokens in tokenized_reviews) / len(tokenized_reviews)\n",
        "print(f\"Average Tokens: {avg_tokens:.2f}\")\n",
        "\n",
        "with open('bpe_merges.txt', 'w') as f:\n",
        "    for pair in merges:\n",
        "        f.write(f\"{pair[0]} {pair[1]}\\n\")\n",
        "print(\"Rules saved...\")\n",
        "\n",
        "# مقایسه با NLTK\n",
        "nltk_tokens = [word_tokenize(review.lower()) for review in sample_reviews]\n",
        "for i, (bpe_tokens, nltk_tokens) in enumerate(zip(tokenized_reviews, nltk_tokens)):\n",
        "    print(f\"\\nReview {i+1}:\")\n",
        "    print(f\"BPE Tokens Count: {len(bpe_tokens)}\")\n",
        "    print(f\"NLTK Tokens Count: {len(nltk_tokens)}\")\n",
        "    print(f\"Few BPE: {bpe_tokens[:10]}...\")\n",
        "    print(f\"Few NLTK: {nltk_tokens[:10]}...\")"
      ],
      "metadata": {
        "id": "9w8JddtLAlBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4qGP_aK6BIsb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}