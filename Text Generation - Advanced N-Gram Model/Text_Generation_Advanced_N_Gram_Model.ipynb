{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+IaEAPsDX6nLgc7DwWt6h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadreza-mohammadi94/NLP-Projects/blob/main/Text%20Generation%20-%20Advanced%20N-Gram%20Model/Text_Generation_Advanced_N_Gram_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libs"
      ],
      "metadata": {
        "id": "CT4vaTpB0G3a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Lzlbwzkz62I",
        "outputId": "56606a34-6c78-4aa9-e4fa-59d1bc996dbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "import requests\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility"
      ],
      "metadata": {
        "id": "FyMaqFZw0Nws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *Load & Preprocess Moby Dick from Project Gutenberg*"
      ],
      "metadata": {
        "id": "Q7l35CKR0RWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_moby_dick():\n",
        "    url = \"https://www.gutenberg.org/files/2701/2701-0.txt\"\n",
        "    response = requests.get(url)\n",
        "    text = response.text\n",
        "\n",
        "    # Optional: Cut off metadata\n",
        "    start = text.find(\"CHAPTER 1. Loomings.\")       # Start of text\n",
        "    end = text.find(\"End of Project Gutenberg\")     # End of text\n",
        "    text = text[start:end]                          # Full text\n",
        "    return text"
      ],
      "metadata": {
        "id": "e4ITpgbl0NEc"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xUOwKV7K0nhT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *Preprocess Text*"
      ],
      "metadata": {
        "id": "zWNA9mhw0oTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # remove punctuation\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = ['<s>', '<s>'] + tokens + ['</s>']\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "iPGz9qE_0p2G"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BNvuscyb09Yz"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *Train Trigram Model*"
      ],
      "metadata": {
        "id": "g6B9bnE70-ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ngram_models(tokens):\n",
        "    trigram_counts = defaultdict(int)\n",
        "    bigram_counts = defaultdict(int)\n",
        "    unigram_counts = defaultdict(int)\n",
        "\n",
        "    for i in range(len(tokens) - 2):\n",
        "        w1, w2, w3 = tokens[i], tokens[i+1], tokens[i+2]\n",
        "        trigram_counts[(w1, w2, w3)] += 1\n",
        "        bigram_counts[(w1, w2)] += 1\n",
        "        unigram_counts[w1] += 1\n",
        "    unigram_counts[tokens[-2]] += 1\n",
        "    unigram_counts[tokens[-1]] += 1\n",
        "\n",
        "    vocab = set(tokens)\n",
        "    return trigram_counts, bigram_counts, unigram_counts, vocab"
      ],
      "metadata": {
        "id": "EL3HTWqj1A6u"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "98p113ih1z6T"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *Interpolated Probability*"
      ],
      "metadata": {
        "id": "j5mljC479Dxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolated_prob(w1, w2, w3, tri, bi, uni, V, l1=0.6, l2=0.3, l3=0.1):\n",
        "    trigram = tri.get((w1, w2, w3), 0)\n",
        "    bigram = bi.get((w2, w3), 0)\n",
        "    unigram = uni.get(w3, 0)\n",
        "\n",
        "    p_tri = trigram / bi.get((w1, w2), 1)\n",
        "    p_bi = bigram / uni.get(w2, 1)\n",
        "    p_uni = unigram / sum(uni.values())\n",
        "\n",
        "    return l1 * p_tri + l2 * p_bi + l3 * p_uni\n"
      ],
      "metadata": {
        "id": "bOhcfIMV9Dmx"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *Compute Perplexity*"
      ],
      "metadata": {
        "id": "Jw0C49Ry11y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_perplexity(tokens, tri, bi, uni, vocab):\n",
        "    log_prob = 0\n",
        "    N = 0\n",
        "    V = len(vocab)\n",
        "\n",
        "    for i in range(2, len(tokens)):\n",
        "        w1, w2, w3 = tokens[i-2], tokens[i-1], tokens[i]\n",
        "        prob = interpolated_prob(w1, w2, w3, tri, bi, uni, V)\n",
        "        log_prob += math.log(prob + 1e-10)  # to prevent log(0)\n",
        "        N += 1\n",
        "    return math.exp(-log_prob / N)\n",
        ""
      ],
      "metadata": {
        "id": "WvXBS15_10jm"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pIvSyhwv10g2"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *Text Generation With Temperature*"
      ],
      "metadata": {
        "id": "Cscw6r-e2fWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(tri, bi, uni, vocab,\n",
        "                  max_words=30, temperature=1.0,\n",
        "                  seed=['<s>', '<s>']):\n",
        "    generated = seed.copy()\n",
        "    V = len(vocab)\n",
        "    for _ in range(max_words):\n",
        "        w1, w2 = generated[-2], generated[-1]\n",
        "        probs = []\n",
        "        candidates = list(vocab)\n",
        "        for w3 in candidates:\n",
        "            p = interpolated_prob(w1, w2, w3, tri, bi, uni, V)\n",
        "            probs.append(p ** (1 / temperature))\n",
        "        total = sum(probs)\n",
        "        probs = [p / total for p in probs]\n",
        "        next_word = random.choices(candidates, weights=probs, k=1)[0]\n",
        "        if next_word == '</s>':\n",
        "            break\n",
        "        generated.append(next_word)\n",
        "    return ' '.join(generated[2:])"
      ],
      "metadata": {
        "id": "Li8Qd2hx10eb"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "izaDGXTH10bu"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "JEVEQOda3mWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the pipeline\n",
        "text = load_moby_dick()\n",
        "tokens = preprocess(text)\n",
        "print(f\"Total Tokens: {len(tokens)}\")\n",
        "\n",
        "trigram_counts, bigram_counts, unigram_counts, vocab = build_ngram_models(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-g_c1T110ZO",
        "outputId": "ab4e0e9e-92c4-4772-8994-77fcafaec45e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 212471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tri, bi, uni, vocab = build_ngram_models(tokens)\n",
        "print(\"Perplexity:\", compute_perplexity(tokens, tri, bi, uni, vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwkARpKC10Wm",
        "outputId": "82c81b9c-eaca-4cd3-f847-88c78c76a87b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 4.952905608522148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "print(\"Sample generated texts:\")\n",
        "for temp in [0.7, 1.0, 1.5]:\n",
        "    print(f\"\\nTemperature: {temp}\")\n",
        "    print(generate_text(tri, bi, uni, vocab, max_words=40, temperature=temp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl-byHFS4O5E",
        "outputId": "0bee7cf6-de04-4956-f525-49a4f1fbb900"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample generated texts:\n",
            "\n",
            "Temperature: 0.7\n",
            "chapter loomings call me that he can not be as it seemed madness it flew from right to them for theirs and that the modern to the captain for some few hands are wanted from the fiery dart that he\n",
            "\n",
            "Temperature: 1.0\n",
            "only consist in hard words were spoken is involuntary consternation commanded three soaked biscuits ye know not even of the tackles may hold by oh sir the hatchway and peered down from this if i touching plenty with my timber\n",
            "\n",
            "Temperature: 1.5\n",
            "awhaling fastened beloved fellowcreatures have after such escapes lord hammer the course the head his unmomentous matter this internal carekilling melted naturally conceit climbing the circumference duelled yarman captains overboarddown comes slowly stealthily ignited his legs coming but and plastertied\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QxJDIV_a4Ui0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}