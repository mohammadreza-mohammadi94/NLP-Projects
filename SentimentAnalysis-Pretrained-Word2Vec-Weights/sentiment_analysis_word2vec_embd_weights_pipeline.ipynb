{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPI0WFOA1IM7e2Y0QA8hhR1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadreza-mohammadi94/NLP-Projects/blob/main/SentimentAnalysis-Pretrained-Word2Vec-Weights/sentiment_analysis_word2vec_embd_weights_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "mIqzVoobkULJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT1dg_EnkS4-",
        "outputId": "d7c3f43b-8594-4d4d-c548-c46b82e34a3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import gensim.downloader as api\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout"
      ],
      "metadata": {
        "id": "y1or7MCPkV9g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis Pipeline"
      ],
      "metadata": {
        "id": "Q5_txgrskY1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentPipeline:\n",
        "    \"\"\"\n",
        "    A professional pipeline for Sentiment Analysis using Pre-trained Word2Vec (Transfer Learning).\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initialize the pipeline with configuration parameters\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.tokenizer = None\n",
        "        self.embedding_matrix = None\n",
        "        self.model = None\n",
        "        self.w2v_model = None\n",
        "\n",
        "        # Data\n",
        "        self.sentences = None\n",
        "        self.labels = None\n",
        "        self.padded_sequences = None\n",
        "        print(\"Pipeline Initialized\")\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"\n",
        "        Download and load the UCI Sentiment Dataset\n",
        "        \"\"\"\n",
        "        print(\"[1/6] Downloading and Loading Data...\")\n",
        "        url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\"\n",
        "\n",
        "        try:\n",
        "            r = requests.get(url)\n",
        "            z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "            with z.open(\"sentiment labelled sentences/amazon_cells_labelled.txt\") as f:\n",
        "                df = pd.read_csv(f, sep='\\t', names=['sentence', 'label'], quoting=3)\n",
        "            self.sentences = df['sentence'].values\n",
        "            self.labels = df['label'].values\n",
        "            print(f\"Data Loaded successfully. Shape: {df.shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data: {e}\")\n",
        "\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        \"\"\"\n",
        "        Tokenization & Padding\n",
        "        \"\"\"\n",
        "        print(\"[2/6] Preprocessing Data...\")\n",
        "        self.tokenizer = Tokenizer(num_words=self.config['max_vocab_size'], oov_token=\"<OOV>\")\n",
        "        self.tokenizer.fit_on_texts(self.sentences)\n",
        "\n",
        "        # Convert to sequences\n",
        "        sequences = self.tokenizer.texts_to_sequences(self.sentences)\n",
        "\n",
        "        # Pad sequences\n",
        "        self.padded_sequences = pad_sequences(\n",
        "            sequences,\n",
        "            maxlen=self.config['max_seq_length'],\n",
        "            padding='post', truncating='post')\n",
        "\n",
        "        self.word_index = self.tokenizer.word_index\n",
        "        print(f\"Preprocessing complete. Unique tokens: {len(self.word_index)}\")\n",
        "\n",
        "\n",
        "    def load_embeddings(self):\n",
        "        \"\"\"\n",
        "        Load google's pre-trained word2vec model.\n",
        "        \"\"\"\n",
        "        print(\"[3/6] Loading Embeddings...\")\n",
        "        self.w2v_model = api.load('word2vec-google-news-300')\n",
        "        print(\"Embeddings Loaded\")\n",
        "\n",
        "\n",
        "    def create_embedding_matrix(self):\n",
        "        \"\"\"\n",
        "        Create the weight matrix for keras embedding layer\n",
        "        \"\"\"\n",
        "        print(\"[4/6] Creating Embedding Matrix...\")\n",
        "        num_words = min(self.config['max_vocab_size'], len(self.word_index) + 1)\n",
        "        embedding_dim = self.config['embedding_dim']\n",
        "\n",
        "        self.embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "        hits, misses = 0, 0\n",
        "\n",
        "        for word, i in self.word_index.items():\n",
        "            if i >= self.config['max_vocab_size']:\n",
        "                continue\n",
        "\n",
        "            if word in self.w2v_model:\n",
        "                self.embedding_matrix[i] = self.w2v_model[word]\n",
        "                hits += 1\n",
        "            else:\n",
        "                misses += 1\n",
        "\n",
        "        print(f\"Matrix created. Hits: {hits}, Misses: {misses}\")\n",
        "        print(f\"Knowledge transfer rate: {hits / (hits + misses) * 100:.2f}%\")\n",
        "\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Define the Keras architecture.\n",
        "        \"\"\"\n",
        "        print(\"[5/6] Building Neural Network...\")\n",
        "\n",
        "        num_words = self.embedding_matrix.shape[0]\n",
        "\n",
        "        self.model = Sequential([\n",
        "            # Pre-trained Embedding Layer (Frozen)\n",
        "            Embedding(\n",
        "                input_dim=num_words,\n",
        "                output_dim=self.config['embedding_dim'],\n",
        "                input_length=self.config['max_seq_length'],\n",
        "                weights=[self.embedding_matrix],\n",
        "                trainable=False\n",
        "            ),\n",
        "            GlobalAveragePooling1D(),\n",
        "            Dense(24, activation='relu'),\n",
        "            Dropout(0.5),\n",
        "            Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        self.model.summary()\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Train the model\n",
        "        \"\"\"\n",
        "        print(\"[6/6] Starting Training....\")\n",
        "        self.history = self.model.fit(\n",
        "            self.padded_sequences,\n",
        "            self.labels,\n",
        "            epochs=self.config['epochs'],\n",
        "            batch_size=self.config['batch_size'],\n",
        "            validation_split=0.2,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Execute the full training pipeline.\n",
        "        \"\"\"\n",
        "        self.load_data()\n",
        "        self.preprocess_data()\n",
        "        self.load_embeddings()\n",
        "        self.create_embedding_matrix()\n",
        "        self.build_model()\n",
        "        self.train()\n",
        "\n",
        "\n",
        "    def predict(self, text):\n",
        "        \"\"\"\n",
        "        Inference method for new data.\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            print(\"Error: Model is not trained yet.\")\n",
        "            return\n",
        "\n",
        "        seq = self.tokenizer.texts_to_sequences([text])\n",
        "        pad = pad_sequences(seq, maxlen=self.config['max_seq_length'], padding='post', truncating='post')\n",
        "\n",
        "        score = self.model.predict(pad, verbose=0)[0][0]\n",
        "        label = \"POSITIVE\" if score > 0.5 else \"NEGATIVE\"\n",
        "\n",
        "        return label, score"
      ],
      "metadata": {
        "id": "o2gEDYJjkYGQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Execution\n",
        "# Config\n",
        "config = {\n",
        "    'max_vocab_size': 5000,\n",
        "    'max_seq_length': 50,\n",
        "    'embedding_dim': 300,\n",
        "    'epochs': 20,\n",
        "    'batch_size': 32\n",
        "}\n",
        "\n",
        "# Instantiate and run pipeline\n",
        "pipeline = SentimentPipeline(config)\n",
        "pipeline.run()\n",
        "\n",
        "# Test with custom sentences\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"TESTING THE MODEL\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "test_sentences = [\n",
        "    \"I absolutely loved this movie, it was fantastic!\",\n",
        "    \"It was a complete waste of time and money.\",\n",
        "    \"The acting was okay but the plot was boring.\",\n",
        "    \"Highly recommended for everyone.\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    label, score = pipeline.predict(sentence)\n",
        "    print(f\"Text: '{sentence}'\")\n",
        "    print(f\"Prediction: {label} ({score:.4f})\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "INpLvNisnTfE",
        "outputId": "dd2daa81-630d-4db7-b09b-06e1c69eb56a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline Initialized\n",
            "[1/6] Downloading and Loading Data...\n",
            "Data Loaded successfully. Shape: (1000, 2)\n",
            "[2/6] Preprocessing Data...\n",
            "Preprocessing complete. Unique tokens: 1879\n",
            "[3/6] Loading Embeddings...\n",
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "Embeddings Loaded\n",
            "[4/6] Creating Embedding Matrix...\n",
            "Matrix created. Hits: 1758, Misses: 121\n",
            "Knowledge transfer rate: 93.56%\n",
            "[5/6] Building Neural Network...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │       \u001b[38;5;34m564,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">564,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m564,000\u001b[0m (2.15 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">564,000</span> (2.15 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m564,000\u001b[0m (2.15 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">564,000</span> (2.15 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6/6] Starting Training....\n",
            "Epoch 1/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.5136 - loss: 0.6937 - val_accuracy: 0.6700 - val_loss: 0.6895\n",
            "Epoch 2/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6189 - loss: 0.6876 - val_accuracy: 0.6600 - val_loss: 0.6854\n",
            "Epoch 3/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6628 - loss: 0.6814 - val_accuracy: 0.7050 - val_loss: 0.6794\n",
            "Epoch 4/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6844 - loss: 0.6737 - val_accuracy: 0.6850 - val_loss: 0.6735\n",
            "Epoch 5/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7169 - loss: 0.6625 - val_accuracy: 0.7200 - val_loss: 0.6646\n",
            "Epoch 6/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7115 - loss: 0.6536 - val_accuracy: 0.7050 - val_loss: 0.6570\n",
            "Epoch 7/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6917 - loss: 0.6487 - val_accuracy: 0.7250 - val_loss: 0.6477\n",
            "Epoch 8/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7064 - loss: 0.6342 - val_accuracy: 0.7450 - val_loss: 0.6382\n",
            "Epoch 9/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7492 - loss: 0.6233 - val_accuracy: 0.7400 - val_loss: 0.6297\n",
            "Epoch 10/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7386 - loss: 0.6169 - val_accuracy: 0.7400 - val_loss: 0.6197\n",
            "Epoch 11/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7438 - loss: 0.6055 - val_accuracy: 0.7400 - val_loss: 0.6101\n",
            "Epoch 12/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7393 - loss: 0.5841 - val_accuracy: 0.7600 - val_loss: 0.5994\n",
            "Epoch 13/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7672 - loss: 0.5709 - val_accuracy: 0.7600 - val_loss: 0.5901\n",
            "Epoch 14/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7698 - loss: 0.5586 - val_accuracy: 0.7750 - val_loss: 0.5801\n",
            "Epoch 15/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7843 - loss: 0.5697 - val_accuracy: 0.7750 - val_loss: 0.5715\n",
            "Epoch 16/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7546 - loss: 0.5579 - val_accuracy: 0.7750 - val_loss: 0.5630\n",
            "Epoch 17/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7573 - loss: 0.5573 - val_accuracy: 0.7900 - val_loss: 0.5538\n",
            "Epoch 18/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8082 - loss: 0.5293 - val_accuracy: 0.7950 - val_loss: 0.5453\n",
            "Epoch 19/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7913 - loss: 0.5150 - val_accuracy: 0.8000 - val_loss: 0.5375\n",
            "Epoch 20/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8150 - loss: 0.5134 - val_accuracy: 0.8000 - val_loss: 0.5298\n",
            "\n",
            "==============================\n",
            "TESTING THE MODEL\n",
            "==============================\n",
            "Text: 'I absolutely loved this movie, it was fantastic!'\n",
            "Prediction: POSITIVE (0.7359)\n",
            "\n",
            "Text: 'It was a complete waste of time and money.'\n",
            "Prediction: NEGATIVE (0.3006)\n",
            "\n",
            "Text: 'The acting was okay but the plot was boring.'\n",
            "Prediction: NEGATIVE (0.3479)\n",
            "\n",
            "Text: 'Highly recommended for everyone.'\n",
            "Prediction: POSITIVE (0.6274)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QpxpfbmrnmJs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}